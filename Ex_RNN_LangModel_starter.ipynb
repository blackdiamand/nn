{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "N0vfqI7e7xXZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/GitHub/nn/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Time Traveller (for so it will be convenient to speak of him)\n",
      "was expounding a recondite matter to us. His grey eyes shone and\n",
      "twinkled, and his usually pale face was flushed and animated. The\n",
      "fire burned brightly, and the soft radiance of the incandescent\n",
      "lights in the lilies of silver caught the bubbles that flashed and\n",
      "passed in our glasses. Our chairs, being his patents, embraced and\n",
      "caressed us rather than submitted to be sat upon, and there was that\n",
      "luxurious after-dinner atmosphere when thought roams gracefully\n",
      "free of the trammels of precision. And he put it to us in this\n",
      "way--marking the points with a lean forefinger--as we sat and lazily\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# P1) Analyse the dataset\n",
    "corpus = [line.strip() for line in open('data/TheTimeMachine.txt') if line.strip()][2:]\n",
    "print(\"\\n\".join(corpus[:10]))\n",
    "\n",
    "corpus = [re.sub('[^A-Za-z0-9]+', ' ', line).lower() for line in corpus]\n",
    "corpus = [re.sub(' +', ' ', line) for line in corpus]\n",
    "corpus = [word for line in corpus for word in line.split()]\n",
    "\n",
    "vocab_size = 3000\n",
    "tkn_counter = Counter([word for word in corpus])\n",
    "vocab = {word: idx for idx, (word, _) in enumerate(tkn_counter.most_common(vocab_size))}\n",
    "vocab[\"/UNK\"] = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "34BC8rwd8lqi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random snippet from the corpus.\n",
      "  * Token IDS:\t [ 312   54   27   42  600    3 1472  110   15  108  439    3   18  108\n",
      "   72  130    4  849   51   52  370  187    3 1472 2275  231  182    0\n",
      "  235   17    4 1473   64   37  371  151  130    0  849    7   20 2276\n",
      "   26  188  219   63  140 1462    7    4]\n",
      "  * Words:\t\t course we have no means of staying back for any length of time any more than a savage or an animal has of staying six feet above the ground but a civilized man is better off than the savage in this respect he can go up against gravitation in a\n"
     ]
    }
   ],
   "source": [
    "class TextCorpusDataset(tf.keras.utils.Sequence):\n",
    "    def __init__(self, corpus, vocab, snippet_len=50):\n",
    "        self.corpus = corpus\n",
    "        self.snippet_len = snippet_len\n",
    "        self.vocab = vocab\n",
    "        self.inv_vocab = {idx: word for word, idx in self.vocab.items()}\n",
    "\n",
    "    def convert2idx(self, word_sequence):\n",
    "        return [self.vocab.get(word, self.vocab[\"/UNK\"]) for word in word_sequence]\n",
    "\n",
    "    def convert2words(self, idx_sequence):\n",
    "        return [self.inv_vocab[idx] for idx in idx_sequence]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.corpus) - self.snippet_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        snippet = self.corpus[idx:idx+self.snippet_len]\n",
    "        snippet = np.array(self.convert2idx(snippet))\n",
    "        return snippet\n",
    "\n",
    "dataset = TextCorpusDataset(corpus, vocab, snippet_len=50)\n",
    "snippet = dataset[1234]\n",
    "print(\"\\nRandom snippet from the corpus.\")\n",
    "print(\"  * Token IDS:\\t\", snippet)\n",
    "print(\"  * Words:\\t\\t\", \" \".join([dataset.inv_vocab[i] for i in snippet]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 2260), ('i', 1266), ('and', 1245), ('of', 1155), ('a', 816), ('to', 695), ('was', 552), ('in', 541), ('that', 443), ('my', 440)]\n",
      "[('the', 2260), ('i', 1266), ('and', 1245), ('of', 1155), ('a', 816), ('to', 695), ('was', 552), ('in', 541), ('that', 443), ('my', 440), ('it', 437), ('had', 354), ('me', 281), ('as', 270), ('at', 243), ('for', 221), ('with', 216), ('but', 204), ('time', 199), ('were', 158), ('this', 152), ('you', 137), ('on', 137), ('then', 134), ('his', 129), ('there', 127), ('he', 123), ('have', 122), ('they', 122), ('from', 122), ('one', 120), ('all', 118), ('not', 114), ('into', 114), ('upon', 113), ('little', 113), ('so', 112), ('is', 106), ('came', 105), ('by', 102), ('some', 94), ('be', 93), ('no', 92), ('could', 92), ('their', 91), ('said', 89), ('saw', 88), ('down', 87), ('them', 86), ('which', 85)]\n",
      "4578\n"
     ]
    }
   ],
   "source": [
    "#What are the 10 most common words found in the text?\n",
    "print(tkn_counter.most_common(10))\n",
    "#What is the most common noun found in the text? time\n",
    "print(tkn_counter.most_common(50))\n",
    "#How many words were found in the text?\n",
    "print(len(tkn_counter))\n",
    "\n",
    "#Should the dictionary contain all words or not? Discuss the advantages and disadvantages of a large vocabulary \n",
    "#More training data is better, of course\n",
    "#Unless your data is poisoned or bad, or your GPU has no RAM\n",
    "#possible disadvantages: overfitting or non-generalizability\n",
    "\n",
    "#What are the disadvantages of using full words as tokens? \n",
    "#Does not express similarity between words\n",
    "#Does not account for structure of words (for example, compound words)\n",
    "#Varying lengths of words \n",
    "\n",
    "#What alternative\n",
    "#tokenization strategies could be used to address such disadvantages?\n",
    "#Word2vec, BERT to transform into fixed-len vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "K8qwrweY8n66"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 6.4364\n",
      "to us in this way  | Pred: in\n",
      "you will soon admit as  | Pred: the\n",
      "that does not last for  | Pred: little\n",
      "that our consciousness moves intermittently  | Pred: i\n",
      "time and any of the  | Pred: the\n",
      "three dimensions particularly why not  | Pred: i\n",
      "brows he lapsed into an  | Pred: brushes\n",
      "four dimensioned being which is  | Pred: of\n",
      "traced such a line and  | Pred: the\n",
      "admit we move freely in  | Pred: the\n",
      "gone wrong we are always  | Pred: of\n",
      "time for instance if i  | Pred: the\n",
      "accelerate his drift along the  | Pred: whole\n",
      "travel indifferently in any direction  | Pred: eloi\n",
      "you for the little go  | Pred: time\n",
      "experiment anyhow said the psychologist  | Pred: in\n",
      "and filby s anecdote collapsed  | Pred: the\n",
      "a chair and sat down  | Pred: the\n",
      "right the psychologist from the  | Pred: filby\n",
      "that it looks singularly askew  | Pred: the\n",
      "sends the machine gliding into  | Pred: the\n",
      "the time traveller put forth  | Pred: and\n",
      "became indistinct was seen as  | Pred: the\n",
      "other look here said the  | Pred: and\n",
      "own account you mean to  | Pred: i\n",
      "have been visible when we  | Pred: and\n",
      "see it nor can we  | Pred: and\n",
      "said laughing we sat and  | Pred: the\n",
      "how we all followed him  | Pred: the\n",
      "trick like that ghost you  | Pred: the\n",
      "that you saw all round  | Pred: the\n",
      "the serious people who took  | Pred: of\n",
      "the model that i remember  | Pred: of\n",
      "and his watch in the  | Pred: problems\n",
      "the psychologist was the only  | Pred: of\n",
      "trick we had witnessed that  | Pred: the\n",
      "his hair disordered and as  | Pred: of\n",
      "word but came painfully to  | Pred: and\n",
      "it off at a draught  | Pred: in\n",
      "was all right the editor  | Pred: in\n",
      "minute perhaps my mind was  | Pred: the\n",
      "man who rang the bell  | Pred: in\n",
      "account of our previous meeting  | Pred: the\n",
      "after to morrow reports the  | Pred: the\n",
      "a treat it is to  | Pred: of\n",
      "man who had been staring  | Pred: the\n",
      "regularity and determination out of  | Pred: ones\n",
      "easy chair and naming the  | Pred: the\n",
      "ve lived eight days such  | Pred: i\n",
      "the inadequacy of pen and  | Pred: the\n",
      "we glanced now and again  | Pred: and\n",
      "that one of the nickel  | Pred: in\n",
      "took the starting lever in  | Pred: the\n",
      "went off with a thud  | Pred: nineteenth\n",
      "again faster and faster still  | Pred: mile\n",
      "the sky leaping it every  | Pred: of\n",
      "presently as i went on  | Pred: the\n",
      "grey and dim i saw  | Pred: of\n",
      "minute the white snow flashed  | Pred: the\n",
      "of impressions grew up in  | Pred: time\n",
      "without any wintry intermission even  | Pred: the\n",
      "bringing my atoms into such  | Pred: ones\n",
      "sickly jarring and swaying of  | Pred: the\n",
      "overset machine everything still seemed  | Pred: of\n",
      "see you presently i thought  | Pred: the\n",
      "the sides were spread so  | Pred: of\n",
      "my eyes from it for  | Pred: valley\n",
      "world savage animal only the  | Pred: the\n",
      "intense blue of the summer  | Pred: in\n",
      "onset and turned over it  | Pred: the\n",
      "white sphinx were the heads  | Pred: and\n",
      "creature but indescribably frail his  | Pred: from\n",
      "them in a strange and  | Pred: in\n",
      "in this at all alarming  | Pred: the\n",
      "in motion and put these  | Pred: i\n",
      "and this may seem egotism  | Pred: the\n",
      "of thunder for a moment  | Pred: of\n",
      "from the sun in a  | Pred: problems\n",
      "with melodious applause and presently  | Pred: morlocks\n",
      "the memory of my confident  | Pred: of\n",
      "the spread of the waxen  | Pred: of\n",
      "i dressed in dingy nineteenth  | Pred: the\n",
      "to and fro of past  | Pred: the\n",
      "peel and stalks and so  | Pred: of\n",
      "effect was extremely rich and  | Pred: the\n",
      "sheep dogs had followed the  | Pred: i\n",
      "attempt to learn the speech  | Pred: in\n",
      "make the exquisite little sounds  | Pred: the\n",
      "i never met people more  | Pred: i\n",
      "the sunlit world again as  | Pred: of\n",
      "building i had left was  | Pred: the\n",
      "condition of ruinous splendour in  | Pred: the\n",
      "very strange experience the first  | Pred: wood\n",
      "thought i looked at the  | Pred: the\n",
      "the miniatures of their parents  | Pred: of\n",
      "than a blessing to the  | Pred: brushes\n",
      "little structure like a well  | Pred: of\n",
      "moss the arm rests cast  | Pred: the\n",
      "the variegated greenery some in  | Pred: time\n",
      "only a glimpse of one  | Pred: the\n",
      "on to a climax one  | Pred: the\n",
      "of wholesome plants leaving the  | Pred: i\n",
      "in spite of the eddies  | Pred: of\n",
      "the ideal of preventive medicine  | Pred: the\n",
      "gone it was natural on  | Pred: the\n",
      "decision and the institution of  | Pred: the\n",
      "/UNK my /UNK in a  | Pred: the\n",
      "be /UNK to a civilized  | Pred: in\n",
      "no doubt the exquisite beauty  | Pred: of\n",
      "sunlight so much was left  | Pred: the\n",
      "than kept /UNK that would  | Pred: traveller\n",
      "to the figure of the  | Pred: problems\n",
      "me but you cannot the  | Pred: and\n",
      "warm /UNK down my cheek  | Pred: and\n",
      "young man i /UNK aloud  | Pred: the\n",
      "me /UNK the sphinx upon  | Pred: the\n",
      "the /UNK of the levers  | Pred: of\n",
      "and /UNK from the broken  | Pred: of\n",
      "enough coming suddenly out of  | Pred: the\n",
      "fear for /UNK from their  | Pred: glare\n",
      "world i must have /UNK  | Pred: the\n",
      "my arm i sat up  | Pred: the\n",
      "the method of my loss  | Pred: of\n",
      "made me desire an /UNK  | Pred: in\n",
      "it was a foolish impulse  | Pred: the\n",
      "it was not a mere  | Pred: in\n",
      "people coming through the bushes  | Pred: in\n",
      "looking little /UNK in white  | Pred: the\n",
      "i /UNK with my fist  | Pred: of\n",
      "at last hot and tired  | Pred: and\n",
      "if they don t you  | Pred: and\n",
      "of it i had made  | Pred: the\n",
      "a day or two things  | Pred: of\n",
      "the bronze doors under the  | Pred: i\n",
      "tree /UNK here and there  | Pred: and\n",
      "/UNK darkness i could see  | Pred: and\n",
      "here and there upon the  | Pred: the\n",
      "/UNK during my time in  | Pred: the\n",
      "/UNK and /UNK /UNK of  | Pred: the\n",
      "i fear i can convey  | Pred: of\n",
      "none i must confess that  | Pred: the\n",
      "of a /UNK tendency there  | Pred: eloi\n",
      "i put it suppose you  | Pred: i\n",
      "rather swiftly but not too  | Pred: the\n",
      "left her i had got  | Pred: the\n",
      "/UNK my /UNK of the  | Pred: the\n",
      "child she wanted to be  | Pred: and\n",
      "her devotion nevertheless she was  | Pred: time\n",
      "and i would watch for  | Pred: the\n",
      "i discovered then among other  | Pred: and\n",
      "of the nights of our  | Pred: the\n",
      "but i felt restless and  | Pred: the\n",
      "and up the hill i  | Pred: i\n",
      "eastern sky grew brighter and  | Pred: of\n",
      "thousand years hence and it  | Pred: of\n",
      "golden age i cannot account  | Pred: and\n",
      "than we know it well  | Pred: traveller\n",
      "a pair of eyes luminous  | Pred: of\n",
      "and touched something soft at  | Pred: the\n",
      "hair on its head and  | Pred: the\n",
      "vanished down the shaft i  | Pred: the\n",
      "not know how long i  | Pred: and\n",
      "ventilation i began to suspect  | Pred: the\n",
      "in their /UNK sport across  | Pred: time\n",
      "so presently i left them  | Pred: the\n",
      "new view plainly this second  | Pred: of\n",
      "flight towards dark shadow and  | Pred: and\n",
      "the notion was so plausible  | Pred: of\n",
      "to you and /UNK incredible  | Pred: the\n",
      "time therein till in the  | Pred: and\n",
      "is due to the length  | Pred: in\n",
      "have to pay /UNK and  | Pred: the\n",
      "dreamed of took a different  | Pred: the\n",
      "it is the most plausible  | Pred: the\n",
      "i could imagine that the  | Pred: and\n",
      "questions and presently she refused  | Pred: the\n",
      "before i could follow up  | Pred: the\n",
      "or twice i had a  | Pred: and\n",
      "these days i had the  | Pred: and\n",
      "and further /UNK in my  | Pred: and\n",
      "use and i was minded  | Pred: the\n",
      "of time and started out  | Pred: i\n",
      "and running to me she  | Pred: i\n",
      "being adapted to the needs  | Pred: of\n",
      "a star was visible while  | Pred: the\n",
      "me a /UNK /UNK in  | Pred: the\n",
      "up in the darkness i  | Pred: the\n",
      "so soon as i struck  | Pred: the\n",
      "machinery grow louder presently the  | Pred: i\n",
      "and the faint /UNK of  | Pred: the\n",
      "wriggling red /UNK in the  | Pred: the\n",
      "it was i stood there  | Pred: the\n",
      "was a /UNK now as  | Pred: the\n",
      "home to me very vividly  | Pred: in\n",
      "a scrap of paper from  | Pred: of\n",
      "/UNK pinkish grey eyes as  | Pred: and\n",
      "out but i had my  | Pred: and\n",
      "several times my head swam  | Pred: the\n",
      "of /UNK escape but that  | Pred: i\n",
      "whose enemy would come upon  | Pred: i\n",
      "foul /UNK it might be  | Pred: and\n",
      "had come at last to  | Pred: and\n",
      "the ease and the sunshine  | Pred: of\n",
      "what it was at the  | Pred: and\n",
      "lay /UNK i felt i  | Pred: i\n",
      "weena like a child upon  | Pred: the\n",
      "pale yellow of the sky  | Pred: in\n",
      "pocket and /UNK placed two  | Pred: the\n",
      "in the trees to me  | Pred: the\n",
      "of their /UNK as a  | Pred: blaze\n",
      "down a long slope into  | Pred: the\n",
      "before me i hesitated at  | Pred: the\n",
      "a danger i did not  | Pred: the\n",
      "now and then a stir  | Pred: in\n",
      "than our own green /UNK  | Pred: traveller\n",
      "/UNK all the /UNK all  | Pred: the\n",
      "her face white and /UNK  | Pred: and\n",
      "behind and /UNK it and  | Pred: the\n",
      "of black and /UNK we  | Pred: i\n",
      "on rats and such like  | Pred: the\n",
      "which the ant like morlocks  | Pred: the\n",
      "of mind was impossible however  | Pred: i\n",
      "a /UNK at hand for  | Pred: i\n",
      "mind i pursued our way  | Pred: the\n",
      "i never followed up the  | Pred: i\n",
      "door which were open and  | Pred: the\n",
      "beside it in the thick  | Pred: in\n",
      "latter day south /UNK here  | Pred: i\n",
      "/UNK removed by the morlocks  | Pred: in\n",
      "of green porcelain had a  | Pred: and\n",
      "in my mind and set  | Pred: the\n",
      "spirit a brown dust of  | Pred: the\n",
      "on either side of me  | Pred: the\n",
      "side so suddenly that she  | Pred: the\n",
      "house before each and only  | Pred: the\n",
      "the immediate presence of the  | Pred: solution\n",
      "a lever not unlike those  | Pred: of\n",
      "s own descendants but it  | Pred: i\n",
      "sides of it i presently  | Pred: the\n",
      "philosophical /UNK and my own  | Pred: /UNK\n",
      "her in her own tongue  | Pred: the\n",
      "matches to have escaped the  | Pred: the\n",
      "had once seen done from  | Pred: i\n",
      "long afternoon it would require  | Pred: and\n",
      "corner i saw was charred  | Pred: the\n",
      "model of a /UNK mine  | Pred: the\n",
      "doors and as it proved  | Pred: the\n",
      "had the camphor in my  | Pred: the\n",
      "hoped to find my bar  | Pred: the\n",
      "my arms full of such  | Pred: of\n",
      "morlocks with it while we  | Pred: the\n",
      "if i was to flourish  | Pred: the\n",
      "even when it is /UNK  | Pred: the\n",
      "had i not restrained her  | Pred: the\n",
      "as my eyes grew accustomed  | Pred: in\n",
      "then i seemed to know  | Pred: the\n",
      "struggle began in the darkness  | Pred: in\n",
      "to the ground with a  | Pred: problems\n",
      "times and now i had  | Pred: the\n",
      "and went out i lit  | Pred: of\n",
      "had fallen so instead of  | Pred: the\n",
      "camphor was in the air  | Pred: in\n",
      "my fire had gone out  | Pred: of\n",
      "i struggled up shaking the  | Pred: i\n",
      "cries of them a minute  | Pred: the\n",
      "the wood in front and  | Pred: i\n",
      "for weena but she was  | Pred: of\n",
      "now i was to see  | Pred: the\n",
      "bewilderment at first i did  | Pred: the\n",
      "the flames died down /UNK  | Pred: of\n",
      "of the fire beat on  | Pred: the\n",
      "here and there and again  | Pred: the\n",
      "little body in the forest  | Pred: in\n",
      "damned /UNK still going hither  | Pred: i\n",
      "absolutely lonely again terribly alone  | Pred: of\n",
      "hasty /UNK upon that evening  | Pred: and\n",
      "of the over world people  | Pred: the\n",
      "rich had been assured of  | Pred: the\n",
      "of intelligence that have to  | Pred: the\n",
      "some little thought outside habit  | Pred: the\n",
      "in spite of my /UNK  | Pred: of\n",
      "my pocket and now came  | Pred: and\n",
      "came into my head as  | Pred: the\n",
      "bronze panels suddenly slid up  | Pred: the\n",
      "made a sweeping blow in  | Pred: the\n",
      "in the forest i think  | Pred: the\n",
      "i brought myself to look  | Pred: of\n",
      "darker then though i was  | Pred: and\n",
      "had long since disappeared for  | Pred: i\n",
      "/UNK to its /UNK red  | Pred: the\n",
      "grew visible i stopped very  | Pred: and\n",
      "that covered every projecting point  | Pred: in\n",
      "and living and along the  | Pred: and\n",
      "beyond the sound of its  | Pred: geometry\n",
      "metallic front its back was  | Pred: of\n",
      "a frightful /UNK i turned  | Pred: of\n",
      "there in the sombre light  | Pred: the\n",
      "of /UNK /UNK creeping in  | Pred: i\n",
      "/UNK nearly a /UNK part  | Pred: the\n",
      "main /UNK of that salt  | Pred: the\n",
      "at it and i judged  | Pred: the\n",
      "sun s disk naturally at  | Pred: the\n",
      "of man the /UNK of  | Pred: i\n",
      "rayless obscurity the sky was  | Pred: the\n",
      "the red water of the  | Pred: covered\n",
      "golden again the sky blue  | Pred: the\n",
      "i /UNK the mechanism down  | Pred: i\n",
      "she had /UNK entered just  | Pred: the\n",
      "started from the south east  | Pred: the\n",
      "looking at the /UNK saw  | Pred: the\n",
      "strange /UNK he looked at  | Pred: i\n",
      "was a momentary stillness then  | Pred: the\n",
      "/UNK of /UNK he said  | Pred: the\n",
      "to the lamp and examined  | Pred: the\n",
      "put his hand to his  | Pred: geometry\n",
      "can t stand another that  | Pred: the\n",
      "and /UNK of grass and  | Pred: i\n",
      "certain /UNK told him he  | Pred: and\n",
      "the house i went up  | Pred: of\n",
      "under the other he laughed  | Pred: the\n",
      "to the /UNK specimen and  | Pred: the\n",
      "down the passage to tell  | Pred: the\n",
      "but this /UNK vanished as  | Pred: the\n",
      "no one has come out  | Pred: the\n",
      "fell among the blood /UNK  | Pred: the\n",
      "i for my own part  | Pred: the\n",
      "me the future is still  | Pred: the\n"
     ]
    }
   ],
   "source": [
    "# P2) The CBOW Embeddings\n",
    "\n",
    "class Word2Vec_CBOW(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec_CBOW, self).__init__()\n",
    "        self.embeddings = layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, context):\n",
    "        context_embeds = self.embeddings(context)\n",
    "        #print(context_embeds.shape)\n",
    "        avg_embed = tf.reduce_sum(context_embeds, axis=2)\n",
    "        #print(avg_embed)\n",
    "        #print(avg_embed.shape)\n",
    "        logits = self.linear(avg_embed)\n",
    "        return logits\n",
    "\n",
    "# hyperparam\n",
    "context_len = 2\n",
    "vocab_size = len(dataset.vocab)\n",
    "embedding_dim = 128\n",
    "learning_rate = 5e-3\n",
    "batch_size = 64\n",
    "num_epochs = 1 #TODO\n",
    "\n",
    "# data\n",
    "dataset = TextCorpusDataset(corpus, vocab, snippet_len=2*context_len + 1)\n",
    "train_loader = tf.data.Dataset.from_generator(lambda: iter(dataset), output_signature=tf.TensorSpec(shape=(5,), dtype=tf.int32)).batch(batch_size).shuffle(buffer_size=len(dataset))\n",
    "\n",
    "# model\n",
    "w2v = Word2Vec_CBOW(vocab_size, embedding_dim)\n",
    "criterion = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# train\n",
    "context_idx = [idx for idx in range(2*context_len+1) if idx != context_len]\n",
    "loader_size = int(len(corpus)/batch_size)+1\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for snippet in train_loader:\n",
    "        context = tf.gather(snippet,  indices=context_idx, axis=1)\n",
    "        target = snippet[:, context_len]\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = w2v(context)\n",
    "            #print(logits)\n",
    "            #print(target)\n",
    "            loss = criterion(target, logits)\n",
    "\n",
    "        gradients = tape.gradient(loss, w2v.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, w2v.trainable_variables))\n",
    "        total_loss += loss.numpy() / loader_size\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss:.4f}')\n",
    "\n",
    "# Extract the word embeddings\n",
    "word_embeddings = w2v.embeddings.weights[0].numpy()\n",
    "\n",
    "for i in range(100, len(dataset), 100):\n",
    "    seq = dataset[i]\n",
    "    context = tf.constant(seq[None, context_idx])\n",
    "    pred_logits = w2v(context)\n",
    "    pred = tf.argmax(pred_logits, axis=1).numpy()[0]\n",
    "    print(\" \".join(dataset.convert2words(seq)), f\" | Pred: {dataset.inv_vocab[pred]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "P7A1cJVF96oU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ep000] | Loss 6.714 \t Perplexity  823.971\n",
      "[Ep001] | Loss 6.136 \t Perplexity  462.132\n",
      "[Ep002] | Loss 5.921 \t Perplexity  372.677\n",
      "[Ep003] | Loss 5.818 \t Perplexity  336.465\n",
      "[Ep004] | Loss 5.751 \t Perplexity  314.545\n",
      "[Ep005] | Loss 5.690 \t Perplexity  295.836\n",
      "[Ep006] | Loss 5.630 \t Perplexity  278.545\n",
      "[Ep007] | Loss 5.576 \t Perplexity  264.102\n",
      "[Ep008] | Loss 5.535 \t Perplexity  253.436\n",
      "[Ep009] | Loss 5.509 \t Perplexity  246.946\n"
     ]
    }
   ],
   "source": [
    "# P3) Next-word prediction using CBOW embeddings\n",
    "\n",
    "class NextWordPredictionMLP(tf.keras.Model):\n",
    "    def __init__(self, num_context, embedding, depth=3, hidden_dim=50):\n",
    "        super(NextWordPredictionMLP, self).__init__()\n",
    "        self.embedding = embedding\n",
    "\n",
    "        self.mlp = models.Sequential()\n",
    "        for d in range(depth):\n",
    "            if d == 0:\n",
    "                self.mlp.add(layers.Dense(hidden_dim, input_shape=(num_context * embedding.embeddings.shape[1],)))\n",
    "            elif d == depth - 1:\n",
    "                self.mlp.add(layers.Dense(embedding.embeddings.shape[0]))\n",
    "            else:\n",
    "                self.mlp.add(layers.Dense(hidden_dim))\n",
    "\n",
    "            # self.mlp.add(layers.Dense(embedding.embeddings.shape[0]))\n",
    "            self.mlp.add(layers.BatchNormalization())\n",
    "            self.mlp.add(layers.ReLU())\n",
    "\n",
    "    def call(self, context):\n",
    "        emb = self.embedding(context)\n",
    "        emb_flat = tf.reshape(emb, (tf.shape(emb)[0], -1))\n",
    "        return self.mlp(emb_flat)\n",
    "\n",
    "def train_one_epoch(model, loss_fcn, optimizer, dataloader):\n",
    "    total_loss = 0.\n",
    "    loader_size = len(list(dataloader))\n",
    "\n",
    "    for batch in dataloader:\n",
    "        batch_past = batch[:, :T]\n",
    "        batch_now = batch[:, -1]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred_now = model(batch_past)\n",
    "            loss = loss_fcn(batch_now, pred_now)\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        total_loss += loss.numpy()\n",
    "\n",
    "    total_loss = total_loss / loader_size\n",
    "    return total_loss\n",
    "\n",
    "def fit(model, loss_fcn, dataloader, optimizer, epochs=30):\n",
    "    for ep in range(epochs):\n",
    "        loss = train_one_epoch(model, loss_fcn, optimizer, dataloader)\n",
    "        print(f\"[Ep{ep:03}] | Loss {loss:.3f} \\t Perplexity  {np.exp(loss):.3f}\")\n",
    "\n",
    "T = 10\n",
    "dataset = TextCorpusDataset(corpus, vocab, snippet_len=T+1)\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "dataloader = tf.data.Dataset.from_generator(lambda: iter(dataset), output_signature=tf.TensorSpec(shape=(T+1,), dtype=tf.int32)).batch(32).shuffle(buffer_size=len(dataset))\n",
    "\n",
    "model = NextWordPredictionMLP(T, w2v.embeddings, depth=2, hidden_dim=50)\n",
    "opt = tf.optimizers.Adam(learning_rate=0.0005)\n",
    "loss_fcn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "fit(model, loss_fcn, dataloader, opt, epochs=num_epochs) # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "L5cnR6IXA0sS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: the time traveller for so it will be convenient to\n",
      "in the sky i saw a the sky over the round over the machine then i saw a the sky in the sky then i saw then in a time of time machine i had any time machine in my last in time in my round the time machine i saw in a the time of the time machine they have have to last in my the time in the time machine they had they had in a time machine in any the time in last to all in a in the time traveller then i saw a last in "
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.device('/cpu:0'), tf.GradientTape() as tape:\n",
    "    prompt = \" \".join(corpus[:10])\n",
    "    print(\"PROMPT:\", prompt)\n",
    "    context = tf.constant([dataset.vocab[word] for word in prompt.split()])[tf.newaxis, :]\n",
    "    for _ in range(100):\n",
    "        next_word_logits = model(context)\n",
    "        next_word_idx = tf.argmax(next_word_logits[:, :-1], axis=1, output_type=tf.dtypes.int32)\n",
    "        next_word = dataset.inv_vocab[next_word_idx.numpy()[0]]\n",
    "        context = tf.concat([context[:, 1:], next_word_idx[:, tf.newaxis]], axis=1)\n",
    "        print(next_word, end=' ')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
