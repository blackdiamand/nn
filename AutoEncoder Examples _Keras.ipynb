{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f15558c7",
   "metadata": {
    "id": "f15558c7"
   },
   "source": [
    "## Keras Building Auto Encoder Examples\n",
    "https://blog.keras.io/building-autoencoders-in-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c83da15",
   "metadata": {
    "id": "1c83da15"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/GitHub/nn/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "# This is the size of our encoded representations\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# This is our input image\n",
    "input_img = keras.Input(shape=(784,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = layers.Dense(encoding_dim, activation='relu')(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = layers.Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# This model maps an input to its reconstruction\n",
    "autoencoder = keras.Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b645967",
   "metadata": {
    "id": "5b645967"
   },
   "outputs": [],
   "source": [
    "# This model maps an input to its encoded representation\n",
    "encoder = keras.Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32c4d881",
   "metadata": {
    "id": "32c4d881"
   },
   "outputs": [],
   "source": [
    "# This is our encoded (32-dimensional) input\n",
    "encoded_input = keras.Input(shape=(encoding_dim,))\n",
    "# Retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# Create the decoder model\n",
    "decoder = keras.Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "363d82ea",
   "metadata": {
    "id": "363d82ea"
   },
   "outputs": [],
   "source": [
    "# configure the model to use a per-pixel binary crossentropy loss, and the Adam optimizer:\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "LwOjEAiMUYkM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 321,
     "status": "ok",
     "timestamp": 1709503055186,
     "user": {
      "displayName": "YU HEN HU",
      "userId": "04083449072545068296"
     },
     "user_tz": 360
    },
    "id": "LwOjEAiMUYkM",
    "outputId": "6bded33d-1e00-498f-9794-f1ce02b41332"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 784)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                25120     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 784)               25872     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50992 (199.19 KB)\n",
      "Trainable params: 50992 (199.19 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f139b88",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 688,
     "status": "ok",
     "timestamp": 1709502742999,
     "user": {
      "displayName": "YU HEN HU",
      "userId": "04083449072545068296"
     },
     "user_tz": 360
    },
    "id": "6f139b88",
    "outputId": "1a652abb-7e9e-4f7d-f12b-bd35e3814968"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Input: MINST dataset\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "(x_train, _), (x_test, _) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7288fb43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1709502743232,
     "user": {
      "displayName": "YU HEN HU",
      "userId": "04083449072545068296"
     },
     "user_tz": 360
    },
    "id": "7288fb43",
    "outputId": "16f08d34-997e-4908-f54c-8838c0eeb5c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# normalize all values between 0 and 1 and flatten the 28x28 images into vectors of size 784.\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "710e0eec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 143801,
     "status": "ok",
     "timestamp": 1709502887031,
     "user": {
      "displayName": "YU HEN HU",
      "userId": "04083449072545068296"
     },
     "user_tz": 360
    },
    "id": "710e0eec",
    "outputId": "82865a09-4230-45ab-924d-c315bab56368"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.2800 - val_loss: 0.1907\n",
      "Epoch 2/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1719 - val_loss: 0.1543\n",
      "Epoch 3/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1447 - val_loss: 0.1345\n",
      "Epoch 4/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1293 - val_loss: 0.1220\n",
      "Epoch 5/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1190 - val_loss: 0.1136\n",
      "Epoch 6/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1119 - val_loss: 0.1078\n",
      "Epoch 7/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1069 - val_loss: 0.1036\n",
      "Epoch 8/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1032 - val_loss: 0.1004\n",
      "Epoch 9/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1005 - val_loss: 0.0981\n",
      "Epoch 10/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0984 - val_loss: 0.0965\n",
      "Epoch 11/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0970 - val_loss: 0.0952\n",
      "Epoch 12/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0961 - val_loss: 0.0944\n",
      "Epoch 13/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0954 - val_loss: 0.0940\n",
      "Epoch 14/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0949 - val_loss: 0.0935\n",
      "Epoch 15/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0946 - val_loss: 0.0933\n",
      "Epoch 16/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0944 - val_loss: 0.0930\n",
      "Epoch 17/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0941 - val_loss: 0.0929\n",
      "Epoch 18/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0940 - val_loss: 0.0927\n",
      "Epoch 19/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0938 - val_loss: 0.0926\n",
      "Epoch 20/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0937 - val_loss: 0.0925\n",
      "Epoch 21/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0936 - val_loss: 0.0924\n",
      "Epoch 22/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0935 - val_loss: 0.0923\n",
      "Epoch 23/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0935 - val_loss: 0.0922\n",
      "Epoch 24/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0934 - val_loss: 0.0922\n",
      "Epoch 25/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0933 - val_loss: 0.0922\n",
      "Epoch 26/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0933 - val_loss: 0.0921\n",
      "Epoch 27/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0932 - val_loss: 0.0922\n",
      "Epoch 28/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0932 - val_loss: 0.0920\n",
      "Epoch 29/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0932 - val_loss: 0.0920\n",
      "Epoch 30/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0931 - val_loss: 0.0920\n",
      "Epoch 31/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0931 - val_loss: 0.0920\n",
      "Epoch 32/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0931 - val_loss: 0.0920\n",
      "Epoch 33/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0931 - val_loss: 0.0919\n",
      "Epoch 34/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0930 - val_loss: 0.0920\n",
      "Epoch 35/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0930 - val_loss: 0.0919\n",
      "Epoch 36/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0930 - val_loss: 0.0919\n",
      "Epoch 37/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0930 - val_loss: 0.0919\n",
      "Epoch 38/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0930 - val_loss: 0.0919\n",
      "Epoch 39/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0929 - val_loss: 0.0918\n",
      "Epoch 40/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0929 - val_loss: 0.0918\n",
      "Epoch 41/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0929 - val_loss: 0.0918\n",
      "Epoch 42/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0929 - val_loss: 0.0919\n",
      "Epoch 43/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0929 - val_loss: 0.0918\n",
      "Epoch 44/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0929 - val_loss: 0.0917\n",
      "Epoch 45/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0928 - val_loss: 0.0917\n",
      "Epoch 46/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0928 - val_loss: 0.0917\n",
      "Epoch 47/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0928 - val_loss: 0.0917\n",
      "Epoch 48/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0928 - val_loss: 0.0917\n",
      "Epoch 49/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0928 - val_loss: 0.0917\n",
      "Epoch 50/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0928 - val_loss: 0.0917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2971a1670>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train for 50 epochs\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "765fe960",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1418,
     "status": "ok",
     "timestamp": 1709502888445,
     "user": {
      "displayName": "YU HEN HU",
      "userId": "04083449072545068296"
     },
     "user_tz": 360
    },
    "id": "765fe960",
    "outputId": "8108f193-ec4e-41a8-9515-cfc6716ba940"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 374us/step\n",
      "313/313 [==============================] - 0s 422us/step\n"
     ]
    }
   ],
   "source": [
    "# visualize the reconstructed inputs and the encoded representations.\n",
    "# Encode and decode some digits\n",
    "# Note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06103514",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "executionInfo": {
     "elapsed": 762,
     "status": "ok",
     "timestamp": 1709502889204,
     "user": {
      "displayName": "YU HEN HU",
      "userId": "04083449072545068296"
     },
     "user_tz": 360
    },
    "id": "06103514",
    "outputId": "0a97a6cf-ecaa-42ea-d15c-d17a293c542d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10  # How many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rWXm7dMeWQKR",
   "metadata": {
    "id": "rWXm7dMeWQKR"
   },
   "source": [
    "## Adding a sparsity constraint on the encoded representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "VnSVxbrbWTt3",
   "metadata": {
    "id": "VnSVxbrbWTt3"
   },
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "encoding_dim = 32\n",
    "\n",
    "input_img = keras.Input(shape=(784,))\n",
    "#   Add a Dense layer with a L1 activity regularizer\n",
    "encoded = layers.Dense(encoding_dim, activation='relu',\n",
    "                activity_regularizer=regularizers.l1(10e-5))(input_img)\n",
    "decoded = layers.Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = keras.Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ZlvvNNGCXSLA",
   "metadata": {
    "id": "ZlvvNNGCXSLA"
   },
   "outputs": [],
   "source": [
    "# This model maps an input to its encoded representation\n",
    "encoder = keras.Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "GMrOMFswXSLA",
   "metadata": {
    "id": "GMrOMFswXSLA"
   },
   "outputs": [],
   "source": [
    "# This is our encoded (32-dimensional) input\n",
    "encoded_input = keras.Input(shape=(encoding_dim,))\n",
    "# Retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# Create the decoder model\n",
    "decoder = keras.Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fJQHRNtmXSLA",
   "metadata": {
    "id": "fJQHRNtmXSLA"
   },
   "outputs": [],
   "source": [
    "# configure the model to use a per-pixel binary crossentropy loss, and the Adam optimizer:\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "T8_Dxn8sYEfo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 265769,
     "status": "ok",
     "timestamp": 1709504291159,
     "user": {
      "displayName": "YU HEN HU",
      "userId": "04083449072545068296"
     },
     "user_tz": 360
    },
    "id": "T8_Dxn8sYEfo",
    "outputId": "b81f623a-f1da-47b9-bb26-6cb99a98cd80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.2881 - val_loss: 0.1977\n",
      "Epoch 2/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1784 - val_loss: 0.1614\n",
      "Epoch 3/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1532 - val_loss: 0.1436\n",
      "Epoch 4/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1390 - val_loss: 0.1320\n",
      "Epoch 5/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1290 - val_loss: 0.1236\n",
      "Epoch 6/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1220 - val_loss: 0.1178\n",
      "Epoch 7/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1170 - val_loss: 0.1137\n",
      "Epoch 8/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1134 - val_loss: 0.1107\n",
      "Epoch 9/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1108 - val_loss: 0.1086\n",
      "Epoch 10/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1090 - val_loss: 0.1070\n",
      "Epoch 11/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1077 - val_loss: 0.1059\n",
      "Epoch 12/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1067 - val_loss: 0.1051\n",
      "Epoch 13/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1060 - val_loss: 0.1044\n",
      "Epoch 14/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1053 - val_loss: 0.1039\n",
      "Epoch 15/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1048 - val_loss: 0.1034\n",
      "Epoch 16/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1044 - val_loss: 0.1030\n",
      "Epoch 17/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1039 - val_loss: 0.1026\n",
      "Epoch 18/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1036 - val_loss: 0.1022\n",
      "Epoch 19/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1032 - val_loss: 0.1019\n",
      "Epoch 20/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1029 - val_loss: 0.1017\n",
      "Epoch 21/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1027 - val_loss: 0.1014\n",
      "Epoch 22/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1024 - val_loss: 0.1011\n",
      "Epoch 23/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1022 - val_loss: 0.1010\n",
      "Epoch 24/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1019 - val_loss: 0.1007\n",
      "Epoch 25/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1017 - val_loss: 0.1005\n",
      "Epoch 26/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1015 - val_loss: 0.1003\n",
      "Epoch 27/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1013 - val_loss: 0.1001\n",
      "Epoch 28/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1012 - val_loss: 0.1000\n",
      "Epoch 29/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1010 - val_loss: 0.0999\n",
      "Epoch 30/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1009 - val_loss: 0.0997\n",
      "Epoch 31/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1007 - val_loss: 0.0995\n",
      "Epoch 32/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1006 - val_loss: 0.0994\n",
      "Epoch 33/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1004 - val_loss: 0.0993\n",
      "Epoch 34/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1003 - val_loss: 0.0992\n",
      "Epoch 35/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1002 - val_loss: 0.0990\n",
      "Epoch 36/100\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1001 - val_loss: 0.0989\n",
      "Epoch 37/100\n",
      " 57/235 [======>.......................] - ETA: 0s - loss: 0.1001"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train for 100 epochs\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/nn/.venv/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/GitHub/nn/.venv/lib/python3.9/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/GitHub/nn/.venv/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/GitHub/nn/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/GitHub/nn/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/GitHub/nn/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GitHub/nn/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/GitHub/nn/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/GitHub/nn/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/GitHub/nn/.venv/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m~/GitHub/nn/.venv/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train for 100 epochs\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=100,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w1aJ0qd_bVtB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 175,
     "status": "ok",
     "timestamp": 1709504840399,
     "user": {
      "displayName": "YU HEN HU",
      "userId": "04083449072545068296"
     },
     "user_tz": 360
    },
    "id": "w1aJ0qd_bVtB",
    "outputId": "346526c0-6871-49e1-b7b1-ecf00ad78acd"
   },
   "outputs": [],
   "source": [
    "# due to the sparsity regularization of the encoder activation, the encoded representations,\n",
    "# encoded_imgs.mean() yields a smaller value than without using the sparsity (L1) regularization\n",
    "encoded_imgs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JE5fQjFRYLuW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2060,
     "status": "ok",
     "timestamp": 1709504521244,
     "user": {
      "displayName": "YU HEN HU",
      "userId": "04083449072545068296"
     },
     "user_tz": 360
    },
    "id": "JE5fQjFRYLuW",
    "outputId": "f9a13811-0bd0-47e5-c936-570b9d13f383"
   },
   "outputs": [],
   "source": [
    "# visualize the reconstructed inputs and the encoded representations.\n",
    "# Encode and decode some digits\n",
    "# Note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P0Eyu8ctYPU5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "executionInfo": {
     "elapsed": 1663,
     "status": "ok",
     "timestamp": 1709504527590,
     "user": {
      "displayName": "YU HEN HU",
      "userId": "04083449072545068296"
     },
     "user_tz": 360
    },
    "id": "P0Eyu8ctYPU5",
    "outputId": "ce303976-a913-47a9-e297-c0cd53dbc1d6"
   },
   "outputs": [],
   "source": [
    "# Use Matplotlib to print some output images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10  # How many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wXbha7TIeqS4",
   "metadata": {
    "id": "wXbha7TIeqS4"
   },
   "source": [
    "## Deep autoencoder\n",
    "We do not have to limit ourselves to a single layer as encoder or decoder, we could instead use a stack of layers, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t_N1JOtyevuK",
   "metadata": {
    "id": "t_N1JOtyevuK"
   },
   "outputs": [],
   "source": [
    "input_img = keras.Input(shape=(784,))\n",
    "encoded = layers.Dense(128, activation='relu')(input_img)\n",
    "encoded = layers.Dense(64, activation='relu')(encoded)\n",
    "encoded = layers.Dense(32, activation='relu')(encoded)\n",
    "\n",
    "decoded = layers.Dense(64, activation='relu')(encoded)\n",
    "decoded = layers.Dense(128, activation='relu')(decoded)\n",
    "decoded = layers.Dense(784, activation='sigmoid')(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xo6-wrkje5eK",
   "metadata": {
    "id": "xo6-wrkje5eK"
   },
   "source": [
    "Let's try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5WAq6Pjwe6eI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 419493,
     "status": "ok",
     "timestamp": 1709506278216,
     "user": {
      "displayName": "YU HEN HU",
      "userId": "04083449072545068296"
     },
     "user_tz": 360
    },
    "id": "5WAq6Pjwe6eI",
    "outputId": "37595f79-e196-4ce6-a8fa-a097e1ea00d6"
   },
   "outputs": [],
   "source": [
    "autoencoder = keras.Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=100,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bOpph5gZfDAg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1539,
     "status": "ok",
     "timestamp": 1709506296133,
     "user": {
      "displayName": "YU HEN HU",
      "userId": "04083449072545068296"
     },
     "user_tz": 360
    },
    "id": "bOpph5gZfDAg",
    "outputId": "c6c23d25-0660-4d5b-8d64-3dbb71c07fbc"
   },
   "outputs": [],
   "source": [
    "# visualize the reconstructed inputs and the encoded representations.\n",
    "# Encode and decode some digits\n",
    "# Note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "J_7Rz5Y_fTKI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "executionInfo": {
     "elapsed": 744,
     "status": "ok",
     "timestamp": 1709506300999,
     "user": {
      "displayName": "YU HEN HU",
      "userId": "04083449072545068296"
     },
     "user_tz": 360
    },
    "id": "J_7Rz5Y_fTKI",
    "outputId": "f29fd3c2-71f8-4603-f2f0-6c3d2f5f6250"
   },
   "outputs": [],
   "source": [
    "# Use Matplotlib to print some output images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10  # How many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4kNlJ31ahMYp",
   "metadata": {
    "id": "4kNlJ31ahMYp"
   },
   "source": [
    "One may also use convolutional layers instead of dense layers to realize the auto-encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sZ9xVUuDhcSk",
   "metadata": {
    "id": "sZ9xVUuDhcSk"
   },
   "source": [
    "## Visualize encoder output\n",
    "We can also have a look at the 128-dimensional encoded representations. These representations are 8x4x4, so we reshape them to 4x32 in order to be able to display them as grayscale images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4sjSFQPfhhYZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "executionInfo": {
     "elapsed": 2045,
     "status": "ok",
     "timestamp": 1709506499819,
     "user": {
      "displayName": "YU HEN HU",
      "userId": "04083449072545068296"
     },
     "user_tz": 360
    },
    "id": "4sjSFQPfhhYZ",
    "outputId": "d70d1922-517c-47fb-9c65-3e7cbfb2d717"
   },
   "outputs": [],
   "source": [
    "encoder = keras.Model(input_img, encoded)\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 8))\n",
    "for i in range(1, n + 1):\n",
    "    ax = plt.subplot(1, n, i)\n",
    "    plt.imshow(encoded_imgs[i].reshape((2, 4 * 4)).T)\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1Sw-DxBYVZAL8_e1my9P0irtaXZ9tmTgg",
     "timestamp": 1709513640240
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
