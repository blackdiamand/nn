{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_0hjAclHDoz"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Set the device\n",
        "device = 'GPU' if tf.config.list_physical_devices('GPU') else 'CPU'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to display attention\n",
        "def display_attention(query, keys, attention):\n",
        "    print(f\"Query: {query.numpy()}\")\n",
        "    for i, (key, attn) in enumerate(zip(keys.numpy(), attention.numpy().T)):\n",
        "        print(f\"Key {i}: {key}, Attention Weight: {attn}\")"
      ],
      "metadata": {
        "id": "NL2tyUI8H3On"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attention function\n",
        "def attention(query, keys, values, verbose=False):\n",
        "    # Step 1: Calculate the dot products of each query with all keys\n",
        "    dot_products = ??\n",
        "    if verbose:\n",
        "        print(\"Dot products:\", dot_products)\n",
        "        print(\"Dot products must be a (1, ) matrix (since we have 1 query and 4 keys). Currently:\", dot_products.shape)\n",
        "\n",
        "    # Step 2: Use the tf.nn.softmax function to compute the attention weights\n",
        "    scaling_factor = tf.math.sqrt(tf.cast(query.shape[1], dtype=tf.float32))\n",
        "    scaled_dot_products = dot_products / scaling_factor\n",
        "    attention_weights = ??\n",
        "    if verbose:\n",
        "        print(\"Attention weights:\", attention_weights)\n",
        "\n",
        "    # Step 3: Weight the values by the attention weights and sum them (Use matrix multiplication)\n",
        "    output = ??\n",
        "    if verbose:\n",
        "        print(\"Output of the attention mechanism:\", output)\n",
        "    return output, attention_weights"
      ],
      "metadata": {
        "id": "ECBlbl-0Htci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For simplicity, let's assume we have one query and four keys/values\n",
        "query = tf.constant([[0, 1, 0]], dtype=tf.float32)\n",
        "keys = tf.constant([[0, 1, 0],\n",
        "                   [1, 0, 0],\n",
        "                   [0, 0, 1],\n",
        "                   [0, 1, 0]], dtype=tf.float32)\n",
        "values = tf.constant([[1, 2],\n",
        "                     [0, 3],\n",
        "                     [4, 5],\n",
        "                     [2, 2]], dtype=tf.float32)\n",
        "\n",
        "output, attention_weights = attention(query, keys, values, verbose=True)\n",
        "display_attention(query, keys, attention_weights)"
      ],
      "metadata": {
        "id": "G3RI-ialKNQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fAT4zojxLmxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Self Attention\n",
        "class SelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_dim, attn_dim, output_dim):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        # Step 1: Define the linear layers for processing the input sequence into queries, keys, and values.\n",
        "        self.Q = layers.Dense(??)\n",
        "        self.K = layers.Dense(??)\n",
        "        self.V = layers.Dense(??)\n",
        "\n",
        "    def call(self, x):\n",
        "        # Step 2: Process the input sequence into queries, keys, and values.\n",
        "        q = self.Q(x)\n",
        "        k = self.K(x)\n",
        "        v = self.V(x)\n",
        "\n",
        "        # Step 3: Use the attention function developed before to compute the self-attention output\n",
        "        out, attn_weights = ??\n",
        "        return out"
      ],
      "metadata": {
        "id": "CWGk8PFXHv_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_seq = tf.constant([[1, 0, 1],\n",
        "                        [0, 1, 0],\n",
        "                        [1, 1, 0],\n",
        "                        [0, 0, 1]], dtype=tf.float32)\n",
        "attn = SelfAttention(3, 8, 3)\n",
        "output_seq = attn(input_seq)\n",
        "print(output_seq)"
      ],
      "metadata": {
        "id": "WXyOR2aTHxf9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}